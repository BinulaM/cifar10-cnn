{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TkcgFXkQODHn"
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries for neural networks and PyTorch \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Import CIFAR-10 dataset and transform from torchvision\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dKP_mk-qG6BO",
    "outputId": "89604b9b-fcbb-4305-e33e-061094fa52b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # Try and run code on GPU\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C6lOk7gNMWNu",
    "outputId": "4dc82c38-a75e-479a-de28-3d914f06bc19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:05<00:00, 29272743.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "First image shape: torch.Size([3, 32, 32])\n",
      "First image label: 6\n"
     ]
    }
   ],
   "source": [
    "# Data Augmentation => Improve precision and accuracy\n",
    "# Transform pipelines for training and testing\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),            # Randomly flips image => For symmetry\n",
    "    transforms.RandomCrop(32, padding=4),         # Randomly crops 32x32 slightly\n",
    "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.2),   # Randomly adjusts brightness or contrast\n",
    "    transforms.RandomRotation(15),                # Rotates between +15 and -15 degrees\n",
    "    transforms.ToTensor(),                        # Converts image to Torch Tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) #Normalise image mean and standard deviation for RGB channels\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([ \n",
    "    transforms.ToTensor(),            # Converts image to Torch Tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) #Normalise image mean and standard deviation for RGB channels\n",
    "])\n",
    "\n",
    "training_set = datasets.CIFAR10(root = './data', train=True, transform=train_transform, download=True) # Load CIFAR-10 training set with defined training transforms\n",
    "test_set = datasets.CIFAR10(root = './data', train=False, transform=test_transform, download = True)   # Load CIFAR-10 test set with defined test transforms\n",
    "\n",
    "# Create dataloader for training and test data\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=128, shuffle=True, num_workers=4, pin_memory=True) # Processes 128 images each time, shuffles dataset each epoch, uses 4 subprocesses to load data in parallel and uses 'pin_memory' to speed up transfer to GPU \n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=128, shuffle=False, num_workers=4, pin_memory=True) # Processes 128 images each time, DOESN'T shuffle dataset, uses 4 subprocesses to load data in parallel and uses 'pin_memory' to speed up transfer to GPU\n",
    "\n",
    "\n",
    "#Test => Gets first image and label from the training set to verify loading\n",
    "first_image, first_label = training_set[0]\n",
    "print(\"First image shape:\", first_image.shape)\n",
    "print(\"First image label:\", first_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "b0HeIugow6pp"
   },
   "outputs": [],
   "source": [
    "# Stem: initial conv layer to extract base features\n",
    "class Stem(nn.Module):\n",
    "    def __init__(self, C_in=3, C_out=128):         # Takes 3 channels in (For the Colours RGB), # Outputs 128 channels (feature maps)\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(C_in, C_out, kernel_size=3, padding=1) # Small window that slides over the image with size 3x3\n",
    "        self.bn = nn.BatchNorm2d(C_out)           # Batch Normalisation -> Assist with training\n",
    "        self.g = nn.ReLU()                        # 'g' => Non-linearity \n",
    "        self.dropout = nn.Dropout(0.2)            # Prevents overfitting\n",
    "\n",
    "    def forward(self, X):\n",
    "        x = self.g(self.bn(self.conv(X)))      # 1. Convolution, 2. Normalisation, 3. Activation\n",
    "        return self.dropout(x)                 # Apply dropout\n",
    "\n",
    "\n",
    "# Expert Branch: predicts the Soft Attention Vector = a = [a1, ..., aK] from input X\n",
    "class ExpertBranch(nn.Module):\n",
    "    def __init__(self, C, K=3, r=4):             # C = No. channels, K = No. Convolution Paths, r = Reduction Factor\n",
    "        super().__init__()\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)    # Spatial average pool => Reduces Spatial Dimensions to (1,1)\n",
    "        self.fc1 = nn.Linear(C, C // r)           # Reduce Channels by 'r'\n",
    "        self.fc2 = nn.Linear(C // r, K)           # Takes in output of 'fc1' and projects it to 'K' attention weights\n",
    "\n",
    "    def forward(self, X):\n",
    "        B, C, H, W = X.shape                      # B = Batch, C = Channels, H = Height, W = Width: to store shape of input X\n",
    "        X_prime = self.avgpool(X).view(B, C)      # X_prime = AvgPool(X) and then flattens it\n",
    "        hidden = F.relu(self.fc1(X_prime))        # FC1 + ReLU (activation): Shrinks to smaller version\n",
    "        a = F.softmax(self.fc2(hidden), dim=1)    # FC2 + Softmax => a = [a1, a2..., aK] (Attention Weights)\n",
    "        return a                                  # Returns vector 'a' with 'K' many values all adding up to 1\n",
    "\n",
    "\n",
    "# ConvKBranch: applies K-many convolutional branches and combines them using 'a' (Attention Weights)\n",
    "class ConvKBranch(nn.Module):\n",
    "    def __init__(self, C_in, C_out, K=3):         # C_in = Channels that go in, C_out = Channels that go out, K = Different Convolutional Paths\n",
    "        super().__init__()\n",
    "        self.K = K                                # Storing no. paths to use in for loop\n",
    "        self.ConvK = nn.ModuleList([              # Creates K-many Convolutional Layers that are 3x3\n",
    "            nn.Conv2d(C_in, C_out, kernel_size=3, padding=1) # Padding = 1 => Make it same size as input\n",
    "            for i in range(K)\n",
    "        ])\n",
    "        self.bn = nn.BatchNorm2d(C_out)           # Calculates Batch Normalisation\n",
    "        self.g = nn.ReLU()                        # ReLU Activation => To keep only positive values\n",
    "\n",
    "    def forward(self, X, a):\n",
    "        final = 0                                 # Final output\n",
    "        for k in range(self.K):                   # Loops over all K-many Convolutional Layers\n",
    "            a_k = a[:, k].view(-1, 1, 1, 1)       # Takes 'a', vector from ExpertBranch -> Takes the k'th value for every image in the batch + reshape batch to be multiplied in conv output\n",
    "            final += a_k * self.ConvK[k](X)       # Weighted Sum of K-many Convolutional Layers => Run 'X' through ConvLayer, a_k multiplied by Conv_k(X)\n",
    "        return self.g(self.bn(final))             # Result = BatchNorm of final output + 'g' (activation)\n",
    "\n",
    "\n",
    "# Block: Combines classes 'ExpertBranch' and 'ConvKBranch'\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, C_in, C_out, K=3, r=4): # Takes in no. channels in and out, 'K' = No. convolutional branches, 'r' = Reduction ratio\n",
    "        super().__init__()\n",
    "        self.expert = ExpertBranch(C_in, K, r)         # Generate Soft Attention Vector\n",
    "        self.conv_branch = ConvKBranch(C_in, C_out, K) # Find weighted sum of K-Convolutional Layers => combines both the Expert Branch and All Convolutional Branches\n",
    "\n",
    "    def forward(self, X):\n",
    "        a = self.expert(X)            # Find 'a' (Attention Weights) through Expert Branch\n",
    "        O = self.conv_branch(X, a)    # Find Weighted output using 'ConvKBranch' class\n",
    "        return O                      # Return Weighted Output\n",
    "\n",
    "\n",
    "# Classifier: Classifier for CIFAR-10 (10 classes)\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, C_in, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)         # Average pooling to reduce dimensions to (1,1)\n",
    "        self.flatten = nn.Flatten()                 # Flatten vector to 1-Dimension\n",
    "        self.dropout = nn.Dropout(0.1)              # Adds a minor dropout\n",
    "        self.fc = nn.Linear(C_in, num_classes)      # Final layer\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.pool(X).view(X.size(0), -1) # Adds pooling and reshape to (B,C)\n",
    "        X = self.flatten(X)\n",
    "        X = self.dropout(X)\n",
    "        return self.fc(X) # Outputs logits for 10 classes\n",
    "\n",
    "\n",
    "# Full CNN Model: Stem → Blocks(Expert Branch + K-many Convolutional Layers) → Classifier\n",
    "class Cifar10Model(nn.Module):\n",
    "    def __init__(self, N=6, num_classes=10, K=3): # 6 blocks, 10 classes, 3 Convolutional Layers\n",
    "        super().__init__()\n",
    "        self.stem = Stem(3, 128) # 3 for RGB and 128 feature maps\n",
    "        self.backbone = nn.ModuleList([\n",
    "            Block(128, 128, K=K) for i in range(N)\n",
    "        ])\n",
    "        self.classifier = Classifier(128, num_classes) # For final Classifier\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.stem(X) # For Stem\n",
    "        for B in self.backbone:\n",
    "            X = B(X) # Pass through expert blocks\n",
    "        return self.classifier(X) # Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1702400440653/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " New model with 40.26% accuracy\n",
      "Epoch [1/120] - Train Loss: 1.8075 | Test Loss: 1.7106 | Train Accuracy: 37.71% | Test Accuracy: 40.26% \n",
      " New model with 47.86% accuracy\n",
      "Epoch [2/120] - Train Loss: 1.4958 | Test Loss: 1.4312 | Train Accuracy: 42.89% | Test Accuracy: 47.86% \n",
      " New model with 57.46% accuracy\n",
      "Epoch [3/120] - Train Loss: 1.2939 | Test Loss: 1.1755 | Train Accuracy: 53.21% | Test Accuracy: 57.46% \n",
      " New model with 60.54% accuracy\n",
      "Epoch [4/120] - Train Loss: 1.1554 | Test Loss: 1.1772 | Train Accuracy: 58.30% | Test Accuracy: 60.54% \n",
      "Epoch [5/120] - Train Loss: 1.0595 | Test Loss: 1.1783 | Train Accuracy: 57.59% | Test Accuracy: 59.18% \n",
      " New model with 68.99% accuracy\n",
      "Epoch [6/120] - Train Loss: 0.9750 | Test Loss: 0.8902 | Train Accuracy: 67.25% | Test Accuracy: 68.99% \n",
      "Epoch [7/120] - Train Loss: 0.9194 | Test Loss: 0.9699 | Train Accuracy: 65.46% | Test Accuracy: 66.04% \n",
      " New model with 71.61% accuracy\n",
      "Epoch [8/120] - Train Loss: 0.8615 | Test Loss: 0.8102 | Train Accuracy: 71.11% | Test Accuracy: 71.61% \n",
      " New model with 73.22% accuracy\n",
      "Epoch [9/120] - Train Loss: 0.8140 | Test Loss: 0.7751 | Train Accuracy: 71.98% | Test Accuracy: 73.22% \n",
      " New model with 75.17% accuracy\n",
      "Epoch [10/120] - Train Loss: 0.7679 | Test Loss: 0.7311 | Train Accuracy: 70.70% | Test Accuracy: 75.17% \n"
     ]
    }
   ],
   "source": [
    "# Model Setup\n",
    "model = Cifar10Model(N=6, num_classes=10, K=3).to(device) #6 Blocks, 10 classes, 3 Convolutional Layers\n",
    "log_loss = nn.CrossEntropyLoss() # Loss Function utilised for image classification\n",
    "optimiser = optim.Adam(model.parameters(), lr=0.01) # Adam optimiser with Learning Rate = 0.01\n",
    "\n",
    "# Scheduler: drops LR by factor of 10 after every 60 epochs to aid with accuracy and tuning\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimiser, step_size=60, gamma=0.1)\n",
    "\n",
    "# Training Loop\n",
    "epochs = 120 # Total no. epochs\n",
    "# Arrays to be used when plotting graphs\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "train_accuracies = []\n",
    "\n",
    "best_acc = 0  # Used to save the model with highest accuracy through each epoch\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    model.train() # Sets model to training mode => Enables Dropout and Batch Normalisation\n",
    "    running_loss = 0.0 # Used to find total loss\n",
    "\n",
    "    for inputs, labels in training_loader: # Loops through the minibatches from training data\n",
    "        inputs, labels = inputs.to(device), labels.to(device) # Moves inputs and labels to GPU\n",
    "\n",
    "        optimiser.zero_grad() # Reset gradients for each iteration\n",
    "        outputs = model(inputs) # Forward pass through the model\n",
    "        loss = log_loss(outputs, labels) # Compute Loss between outputs and labels\n",
    "        loss.backward() # Backpropagation: Backward Pass -> Compute gradients\n",
    "        optimiser.step() # Updating weights and biases\n",
    "\n",
    "        running_loss += loss.item() # Accumulating loss\n",
    "\n",
    "    average_loss = running_loss / len(training_loader) # Mean loss over each epoch\n",
    "    train_losses.append(average_loss) # Store loss to be added to graphs\n",
    "\n",
    "    # Evaluate on TRAINING data\n",
    "    model.eval() # Set model to evaluation mode => Disables Dropout and Batch Normalisation\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient calculation => Saves memory\n",
    "        for inputs, labels in training_loader: # Loops through training data again\n",
    "            inputs, labels = inputs.to(device), labels.to(device) # Moves inputs and labels to selected device (GPU/CPU)\n",
    "            outputs = model(inputs) # Forward Pass through model -> Get predictions for batch\n",
    "            _, predicted = torch.max(outputs, dim=1) # Take in the class with the highest score\n",
    "            train_total += labels.size(0) # Sums the total no. labels\n",
    "            train_correct += (predicted == labels).sum().item() # Sums the correct predictions to total\n",
    "\n",
    "    train_accuracy = 100 * train_correct / train_total # Calculate training accuracy\n",
    "    train_accuracies.append(train_accuracy) # Append to array to be used to plot graphs\n",
    "\n",
    "    # Evaluate on TEST data\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient calculation => Saves memory\n",
    "        # Used same logic for evaluating training data for test data\n",
    "        for inputs, labels in test_loader: # Loops through test data\n",
    "            inputs, labels = inputs.to(device), labels.to(device) # Moves inputs and labels to selected device (GPU/CPU)\n",
    "            outputs = model(inputs) # Forward Pass through model -> Get predictions for batch\n",
    "            loss = log_loss(outputs, labels) # Calculate test loss in the batch\n",
    "\n",
    "            test_loss += loss.item() # Sums up batch's loss to total loss\n",
    "            _, predicted = torch.max(outputs, dim=1) # Get predicted class with highest score\n",
    "            test_total += labels.size(0) # Sums the total no. labels\n",
    "            test_correct += (predicted == labels).sum().item() # Sums the correct predictions to total\n",
    "\n",
    "    overall_test_loss = test_loss / len(test_loader) # Calculate average test loss over all batches\n",
    "    test_accuracy = 100 * test_correct / test_total # Calculate accuracy\n",
    "    test_losses.append(overall_test_loss) # Append to array for graph plotting\n",
    "    test_accuracies.append(test_accuracy) # Append to array for graph plotting\n",
    "\n",
    "    # Save the best model\n",
    "    if test_accuracy > best_acc: # Check if current epoch has best test accuracy\n",
    "        best_acc = test_accuracy # Update best accuracy\n",
    "        torch.save(model.state_dict(), 'best_model.pth') # Save model's parameter to file\n",
    "        print(f\" New model with {best_acc:.2f}% accuracy\")\n",
    "\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}] - \"\n",
    "          f\"Train Loss: {average_loss:.4f} | \"\n",
    "          f\"Test Loss: {overall_test_loss:.4f} | \"\n",
    "          f\"Train Accuracy: {train_accuracy:.2f}% | \"\n",
    "          f\"Test Accuracy: {test_accuracy:.2f}% \")\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F3YesAR6YcVV"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Training Loss\n",
    "plt.figure(figsize=(10, 4)) # Width is 10, Height is 4\n",
    "plt.plot(train_losses, label='Training Loss', color='red')\n",
    "plt.plot(test_losses, label='Test Loss', color='purple')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Evolution of Loss over Epochs')\n",
    "plt.grid(True) # Grid to assist with reading data from graph\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Test Accuracy\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(test_accuracies, label='Test Accuracy (%)', color='blue')\n",
    "plt.plot(train_accuracies, label='Train Accuracy (%)', color='green')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Test Accuracy over Epochs')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
